{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddadce0f-daa5-41d2-bb10-ddfa175bbd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e305bc5-1343-4bda-8b7f-6728e3936436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming the dataset is another option, but too slow in my experience.\n",
    "\n",
    "#https://huggingface.co/datasets/laion/laion400m\n",
    "#https://github.com/rom1504/img2dataset/issues/449\n",
    "#https://www.kaggle.com/datasets/romainbeaumont/laion400m\n",
    "#https://github.com/rom1504/img2dataset\n",
    "#if streaming, use high performance dns resolver: https://github.com/rom1504/img2dataset#setting-up-a-high-performance-dns-resolver\n",
    "\n",
    "#how to do text search through hf dataset: https://huggingface.co/docs/dataset-viewer/en/search\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import subprocess\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from img2dataset import download\n",
    "\n",
    "# for my setup, at least, pyarrow needs to be installed from conda-forge rather than pip due to some discrepancy in the versions\n",
    "# and compatibility with HuggingFace.\n",
    "# To keep things within a uv environment, I copied the relevant files from a conda environment into uv's .venv/bin. \n",
    "#try:\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "#except ModuleNotFoundError:\n",
    "    #subprocess.run(\"conda install -c conda-forge -y  datasets pyarrow libparquet\", shell=True)\n",
    "    #from datasets import load_dataset\n",
    "\n",
    "#from huggingface_hub import HfFolder, whoami\n",
    "\n",
    "rng = np.random.default_rng(seed=1234)\n",
    "\n",
    "# uncomment below to sign in to HuggingFace\n",
    "# login can be done with huggingface_hub.login() in Python or huggingface-cli login in CLI\n",
    "#token = HfFolder.get_token()\n",
    "\n",
    "#if token is None:\n",
    "#    try:\n",
    "#        subprocess.run(\"huggingface-cli login\", shell=True)\n",
    "#    except Exception as e:\n",
    "#        print(f\"Unable to call 'huggingface-cli login' in shell: {e}.\")\n",
    "#else:\n",
    "#    try:\n",
    "#        user_info = whoami()\n",
    "#        print(f\"Logged into HuggingFace as: {user_info['name']}\")\n",
    "#    except Exception as e:\n",
    "#        print(f\"Unable to identify user from HuggingFace token: {e}\")\n",
    "#\n",
    "#assert HfFolder.get_token() is not None, \"No authentication token for HuggingFace found, aborting.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "075bdd4f-240c-4488-89c2-1ffaef8474a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_function(to_search, search_terms) -> bool:\n",
    "    if not to_search:\n",
    "        return False\n",
    "    to_search = to_search.lower()\n",
    "    terminators = [\" \", \"\\n\" \".\", \",\", \":\", \";\"]\n",
    "    for term in search_terms:\n",
    "        term = term.lower()\n",
    "        for t in terminators:\n",
    "            if f\" {term}{t}\" in to_search:\n",
    "                return True\n",
    "            elif f\" {term}s{t}\" in to_search: \n",
    "                return True\n",
    "    return False\n",
    "        \n",
    "\n",
    "def search_dataset(dataset: datasets.Dataset, search_terms, column=\"caption\", start_from=0, up_to=10000) -> datasets.Dataset:\n",
    "    # TODO: randomize access to dataset\n",
    "    if isinstance(search_terms, str):\n",
    "        search_terms = [search_terms]\n",
    "    if up_to == \"full\":\n",
    "        up_to = dataset.num_rows\n",
    "    elif up_to == \"half\":\n",
    "        up_to = dataset.num_rows // 2\n",
    "    elif up_to == \"quarter\":\n",
    "        up_to = dataset.num_rows // 4\n",
    "    elif up_to == \"tenth\":\n",
    "        up_to = dataset.num_rows // 10\n",
    "    up_to = min(start_from+up_to, dataset.num_rows)\n",
    "    print(f\"Filtering from row {start_from} to row {up_to} of the dataset\")\n",
    "    return dataset.select([i for i in range(up_to)]).filter(lambda row: filter_function(row[column], search_terms))\n",
    "\n",
    "def get_n(dataset: datasets.Dataset, n: int, exclusions=None) -> (datasets.Dataset, list):\n",
    "    rand_indices = rng.choice(dataset.num_rows, size=n, replace=False)\n",
    "    if exclusions:\n",
    "        assert isinstance(exclusions, np.ndarray), \"excluded indices must be passed as np array\"\n",
    "        while np.intersect1d(rand_indices, exclusions).size > 0:\n",
    "            to_replace = np.intersect1d(rand_indices, exclusions, return_indices=True)[1]\n",
    "            rand_indices[to_replace] = rng.choice(dataset.num_rows, size=to_replace.size, replace=False) \n",
    "            while(np.unique(rand_indices).size != rand_indices.size):\n",
    "                to_keep = np.unique(rand_indices, return_index=True)[1]\n",
    "                mask = np.ones(rand_indices.shape, bool)\n",
    "                mask[to_keep] = False\n",
    "                rand_indices[mask] = rng.choice(dataset.num_rows, size=rand_indices[mask].size, replace=False)\n",
    "    return dataset.select(rand_indices), rand_indices\n",
    "\n",
    "def img_from_dataset(dataset: datasets.Dataset, dest_path: str):\n",
    "    # first save dataset to new parquet file to hidden version of specified path\n",
    "    path = Path(dest_path)\n",
    "    path.mkdir(exist_ok=True)\n",
    "    (path / \"selection\").mkdir(exist_ok=True)\n",
    "    metadata_path = path / \"selection/01.parquet\"\n",
    "    dataset.to_parquet(metadata_path)\n",
    "    output_path = path / \"output\"\n",
    "    download(\n",
    "        processes_count=8, \n",
    "        thread_count=16, \n",
    "        url_list=str(metadata_path),\n",
    "        resize_mode=\"no\",\n",
    "        output_folder=str(output_path),\n",
    "        output_format=\"files\",\n",
    "        input_format=\"parquet\",\n",
    "        url_col=\"url\",\n",
    "        caption_col=\"caption\",\n",
    "        enable_wandb=True,\n",
    "        number_sample_per_shard=1000,\n",
    "        distributor=\"multiprocessing\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79b4dbc3-e3bf-4f97-b5d1-63eec488f1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e01b3273124af389a681995baac630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2366d391f8854489980616d7d89b4150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/128 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdef43d0016b4c96b903ecee1b47c47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e450fb2bde3a49319c3f652e4f16dbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = Path(\"./laion400m\")\n",
    "data_files = [str(file) for file in path.glob(\"part*.parquet\")]\n",
    "dataset = load_dataset(\"parquet\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7fb3559-2043-41a3-8328-d19cea16bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering from row 0 to row 36102061 of the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e61a79a66a49b9b821bc594ad59f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/36102061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_results = search_dataset(dataset[\"train\"], \"car\", up_to=\"tenth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c62cf276-45db-4df4-9397-dbc509fc7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = get_n(search_results, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "190e4590-d002-4bf4-8e96-8a8fff73d4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0591bd1a9b4e4ccfbd3fa4b06502983f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the downloading of this file\n",
      "Sharding file number 1 of 1 called /home/andrew/Documents/raw_images_image_priv/testing_dir/selection/01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sharded in 1 shards\n",
      "Downloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: acmayo to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.8\n",
      "wandb: Run data is saved locally in /home/andrew/Documents/raw_images_image_priv/wandb/run-20250331_180234-gy41xlen\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run winter-gorge-5\n",
      "wandb: â­ï¸ View project at https://wandb.ai/acmayo/img2dataset?apiKey=85a264506652d736bafbfaeeb161c262630fa4b9\n",
      "wandb: ðŸš€ View run at https://wandb.ai/acmayo/img2dataset/runs/gy41xlen?apiKey=85a264506652d736bafbfaeeb161c262630fa4b9\n",
      "wandb: WARNING Do NOT share these links with anyone. They can be used to claim your runs.\n",
      "1it [00:06,  6.10s/it]\n",
      "wandb: WARNING Artifacts logged anonymously cannot be claimed and expire after 7 days.\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:               total/count â–\n",
      "wandb:  total/failed_to_download â–\n",
      "wandb:    total/failed_to_resize â–\n",
      "wandb:         total/img_per_sec â–\n",
      "wandb:             total/success â–\n",
      "wandb:              worker/count â–\n",
      "wandb: worker/failed_to_download â–\n",
      "wandb:   worker/failed_to_resize â–\n",
      "wandb:        worker/img_per_sec â–\n",
      "wandb:            worker/success â–\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:               total/count 100\n",
      "wandb:  total/failed_to_download 0.18\n",
      "wandb:    total/failed_to_resize 0.01\n",
      "wandb:         total/img_per_sec 21.37905\n",
      "wandb:             total/success 0.81\n",
      "wandb:              worker/count 100\n",
      "wandb: worker/failed_to_download 0.18\n",
      "wandb:   worker/failed_to_resize 0.01\n",
      "wandb:        worker/img_per_sec 21.37905\n",
      "wandb:            worker/success 0.81\n",
      "wandb: \n",
      "wandb: ðŸš€ View run winter-gorge-5 at: https://wandb.ai/acmayo/img2dataset/runs/gy41xlen?apiKey=85a264506652d736bafbfaeeb161c262630fa4b9\n",
      "wandb: â­ï¸ View project at: https://wandb.ai/acmayo/img2dataset?apiKey=85a264506652d736bafbfaeeb161c262630fa4b9\n",
      "wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20250331_180234-gy41xlen/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker  - success: 0.810 - failed to download: 0.180 - failed to resize: 0.010 - images per sec: 21 - count: 100\n",
      "total   - success: 0.810 - failed to download: 0.180 - failed to resize: 0.010 - images per sec: 21 - count: 100\n"
     ]
    }
   ],
   "source": [
    "img_from_dataset(selection[0], \"./testing_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83d6bc-55b7-4bb2-b772-342ecf26029e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
